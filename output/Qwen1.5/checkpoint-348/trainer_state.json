{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9951742627345843,
  "eval_steps": 500,
  "global_step": 348,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.08579088471849866,
      "grad_norm": 0.04379373788833618,
      "learning_rate": 9.71264367816092e-05,
      "loss": 3.9186,
      "step": 10
    },
    {
      "epoch": 0.17158176943699732,
      "grad_norm": 0.03543248772621155,
      "learning_rate": 9.425287356321839e-05,
      "loss": 3.1251,
      "step": 20
    },
    {
      "epoch": 0.257372654155496,
      "grad_norm": 0.03391461819410324,
      "learning_rate": 9.137931034482759e-05,
      "loss": 3.0656,
      "step": 30
    },
    {
      "epoch": 0.34316353887399464,
      "grad_norm": 0.0276623722165823,
      "learning_rate": 8.850574712643679e-05,
      "loss": 3.1135,
      "step": 40
    },
    {
      "epoch": 0.4289544235924933,
      "grad_norm": 0.029155587777495384,
      "learning_rate": 8.563218390804599e-05,
      "loss": 3.0823,
      "step": 50
    },
    {
      "epoch": 0.514745308310992,
      "grad_norm": 0.026078585535287857,
      "learning_rate": 8.275862068965517e-05,
      "loss": 3.0572,
      "step": 60
    },
    {
      "epoch": 0.6005361930294906,
      "grad_norm": 0.043532710522413254,
      "learning_rate": 7.988505747126437e-05,
      "loss": 2.9592,
      "step": 70
    },
    {
      "epoch": 0.6863270777479893,
      "grad_norm": 0.042913466691970825,
      "learning_rate": 7.701149425287356e-05,
      "loss": 3.0632,
      "step": 80
    },
    {
      "epoch": 0.7721179624664879,
      "grad_norm": 0.03692713752388954,
      "learning_rate": 7.413793103448277e-05,
      "loss": 3.0343,
      "step": 90
    },
    {
      "epoch": 0.8579088471849866,
      "grad_norm": 0.03575325384736061,
      "learning_rate": 7.126436781609196e-05,
      "loss": 3.0269,
      "step": 100
    },
    {
      "epoch": 0.9436997319034852,
      "grad_norm": 0.02929593436419964,
      "learning_rate": 6.839080459770116e-05,
      "loss": 2.9508,
      "step": 110
    },
    {
      "epoch": 1.0343163538873994,
      "grad_norm": 0.017498252913355827,
      "learning_rate": 6.551724137931034e-05,
      "loss": 3.3717,
      "step": 120
    },
    {
      "epoch": 1.1201072386058981,
      "grad_norm": 0.029289208352565765,
      "learning_rate": 6.264367816091954e-05,
      "loss": 2.9142,
      "step": 130
    },
    {
      "epoch": 1.2058981233243968,
      "grad_norm": 0.03193110227584839,
      "learning_rate": 5.977011494252874e-05,
      "loss": 2.9793,
      "step": 140
    },
    {
      "epoch": 1.2916890080428955,
      "grad_norm": 0.02889823541045189,
      "learning_rate": 5.689655172413794e-05,
      "loss": 2.9626,
      "step": 150
    },
    {
      "epoch": 1.377479892761394,
      "grad_norm": 0.02751368284225464,
      "learning_rate": 5.402298850574713e-05,
      "loss": 2.8527,
      "step": 160
    },
    {
      "epoch": 1.4632707774798928,
      "grad_norm": 0.03441798686981201,
      "learning_rate": 5.1149425287356324e-05,
      "loss": 2.8737,
      "step": 170
    },
    {
      "epoch": 1.5490616621983915,
      "grad_norm": 0.019225509837269783,
      "learning_rate": 4.827586206896552e-05,
      "loss": 2.94,
      "step": 180
    },
    {
      "epoch": 1.63485254691689,
      "grad_norm": 0.03804634138941765,
      "learning_rate": 4.5402298850574716e-05,
      "loss": 2.9308,
      "step": 190
    },
    {
      "epoch": 1.7206434316353887,
      "grad_norm": 0.022897537797689438,
      "learning_rate": 4.252873563218391e-05,
      "loss": 2.8736,
      "step": 200
    },
    {
      "epoch": 1.8064343163538874,
      "grad_norm": 0.023831577971577644,
      "learning_rate": 3.965517241379311e-05,
      "loss": 2.935,
      "step": 210
    },
    {
      "epoch": 1.8922252010723861,
      "grad_norm": 0.03696692734956741,
      "learning_rate": 3.67816091954023e-05,
      "loss": 2.8963,
      "step": 220
    },
    {
      "epoch": 1.9780160857908848,
      "grad_norm": 0.03907659277319908,
      "learning_rate": 3.390804597701149e-05,
      "loss": 2.9131,
      "step": 230
    },
    {
      "epoch": 2.068632707774799,
      "grad_norm": 0.0299063827842474,
      "learning_rate": 3.103448275862069e-05,
      "loss": 3.0388,
      "step": 240
    },
    {
      "epoch": 2.1544235924932975,
      "grad_norm": 0.042489901185035706,
      "learning_rate": 2.8160919540229884e-05,
      "loss": 2.83,
      "step": 250
    },
    {
      "epoch": 2.2402144772117962,
      "grad_norm": 0.026649916544556618,
      "learning_rate": 2.5287356321839083e-05,
      "loss": 2.8316,
      "step": 260
    },
    {
      "epoch": 2.326005361930295,
      "grad_norm": 0.02834256738424301,
      "learning_rate": 2.2413793103448276e-05,
      "loss": 2.9352,
      "step": 270
    },
    {
      "epoch": 2.4117962466487937,
      "grad_norm": 0.04295729845762253,
      "learning_rate": 1.9540229885057475e-05,
      "loss": 2.899,
      "step": 280
    },
    {
      "epoch": 2.4975871313672924,
      "grad_norm": 0.051217421889305115,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 2.8382,
      "step": 290
    },
    {
      "epoch": 2.583378016085791,
      "grad_norm": 0.048414357006549835,
      "learning_rate": 1.3793103448275863e-05,
      "loss": 2.7856,
      "step": 300
    },
    {
      "epoch": 2.66916890080429,
      "grad_norm": 0.06652025133371353,
      "learning_rate": 1.091954022988506e-05,
      "loss": 2.8358,
      "step": 310
    },
    {
      "epoch": 2.754959785522788,
      "grad_norm": 0.03870008513331413,
      "learning_rate": 8.045977011494253e-06,
      "loss": 2.8374,
      "step": 320
    },
    {
      "epoch": 2.840750670241287,
      "grad_norm": 0.031087731942534447,
      "learning_rate": 5.172413793103448e-06,
      "loss": 2.8959,
      "step": 330
    },
    {
      "epoch": 2.9265415549597855,
      "grad_norm": 0.04279060289263725,
      "learning_rate": 2.2988505747126437e-06,
      "loss": 2.8252,
      "step": 340
    }
  ],
  "logging_steps": 10,
  "max_steps": 348,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.848530826153165e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
